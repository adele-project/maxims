{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Mounting the drive content that contains the necessary files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayKxC5zkUQGu",
        "outputId": "3871af9d-7790-4756-b6a2-a4343ab76833"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the necessary modules\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "import string\n",
        "from nltk.corpus import stopwords,wordnet\n",
        "import numpy as np\n",
        "import re\n",
        "import random"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXMl4V4nhHx7",
        "outputId": "59c1b22b-a8ec-40b4-f3d6-ec25cd1cee72"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#obtaining the list of stop words in english\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "O9Q1cJKQhSFD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function that finds the similar words to the input using the WordNet and the property of POS tag\n",
        "def synonyms(wordgiven, ndesired):\n",
        "    #collecting the synsets from the WordNet interface\n",
        "    wn=wordnet.synsets(wordgiven)\n",
        "    words=[]\n",
        "    taglist=[]\n",
        "    #obtaning the POS tag of the word in process\n",
        "    wordtag=nltk.pos_tag([wordgiven])[0][1][0]\n",
        "    check=1\n",
        "    if len(wn)!=0:\n",
        "        for word in wn:\n",
        "            #performing a POS compatibility check on the synset level that result in a prioritization tag\n",
        "            for entry in word.lemmas():\n",
        "                if wordtag=='N' and str(entry).split('.')[1]=='n':\n",
        "                    priortag=1\n",
        "                elif (wordtag in ['J']) and (str(entry).split('.')[1] in ['a','s']):\n",
        "                    priortag=1\n",
        "                elif (wordtag in ['R']) and (str(entry).split('.')[1] in ['r']):\n",
        "                    priortag=1\n",
        "                else:\n",
        "                    priortag=0\n",
        "                if (wordgiven.lower()!=entry.name().lower()) and (entry.name().lower() not in words):\n",
        "                    words.append(entry.name().lower()) #list that contains the results\n",
        "                    taglist.append(priortag) #list that contains the prioritization tags\n",
        "    #reordering of the results in the list based on their prioritization tag\n",
        "    wordsfinal=[None]*len(words)\n",
        "    if len(words)!=0:\n",
        "        indexstart=taglist.count(1)\n",
        "        count1=0\n",
        "        for i in range(0,len(words)):\n",
        "            if taglist[i]==1:\n",
        "                wordsfinal[count1]=words[i]\n",
        "                count1+=1\n",
        "            else:\n",
        "                wordsfinal[indexstart]=words[i]\n",
        "                indexstart+=1\n",
        "        check=1\n",
        "    else:\n",
        "        check=0\n",
        "    return wordsfinal[0:ndesired],check   "
      ],
      "metadata": {
        "id": "lNSLKDerUNxJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function that cleans the lines from undesired characters as tags,punctuation signs etc.\n",
        "def cleanline(line):\n",
        "    tagpattern=r'(<)(.+?)(>)'\n",
        "    numberpattern=r'([0-9]+)'\n",
        "    cl=re.sub(tagpattern,'',line.lower()).replace('\\n','').replace('“','').replace('”','').replace('…','').replace('—',' ').replace('‘','').replace('’','').replace('\\\\' , ' ').replace('/',' ')\n",
        "    cl=re.sub(numberpattern,' ',cl)\n",
        "    for char in cl:\n",
        "            if (char in string.punctuation):\n",
        "                cl=cl.replace(char,' ')\n",
        "    return cl"
      ],
      "metadata": {
        "id": "3KqnFpGYhYhc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path of the file that contains the maxims to be augmented\n",
        "TRAINSETPATHX='/content/drive/MyDrive/Maximsamples.txt'\n",
        "\n",
        "f=open(TRAINSETPATHX,'r',encoding='utf8')\n",
        "maximbits=f.readlines()\n",
        "f.close()\n",
        "\n",
        "#removing the endline characters\n",
        "maximbits=[maxim[:-1] for maxim in maximbits]\n",
        "\n",
        "print(\"The number of maxims: {}\".format(len(maximbits)))"
      ],
      "metadata": {
        "id": "VbNJbPekUUwU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ba47d72-6c2a-43f9-a6c6-0ebbedd7287d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of maxims: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#breaking th maxim lines into sentences\n",
        "sentences=[]\n",
        "nsentence=[] #storing the number of sentences each maxim contains\n",
        "\n",
        "for line in maximbits:\n",
        "    count=0\n",
        "    #splitting sentences using \". \" characters\n",
        "    #listsent=line.replace('\\\\\\n','').split('. ')\n",
        "    listsent=line.split('. ')\n",
        "    for i in range(0,len(listsent)):\n",
        "        sentence=listsent[i]\n",
        "        #sentences that are too short are eliminated\n",
        "        if len(sentence)<5:\n",
        "            continue\n",
        "        elif (i==len(listsent)-1):\n",
        "            #the last one is added as it is since we used the character sequence \". \" for the seperation\n",
        "            #and the last sentence contains only \".\"\n",
        "            sentences.append(sentence)\n",
        "            count=count+1\n",
        "            continue\n",
        "        else:\n",
        "            #the \".\" characters are restored\n",
        "            if(sentence[-1]!='.'):\n",
        "                sentence=sentence+'.'\n",
        "            sentences.append(sentence)\n",
        "            count=count+1\n",
        "    nsentence.append(count)"
      ],
      "metadata": {
        "id": "3I10XlC6UWyI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdYhs1crTkiS",
        "outputId": "e2496bd3-7f2b-4a0d-b556-ac079bb13c55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n"
          ]
        }
      ],
      "source": [
        "perfix=[0.6] #percentage desired\n",
        "noisesent6=[]\n",
        "for sent in sentences:\n",
        "    print(sentences.index(sent))\n",
        "    #tokinizing and capitalizing the sentences\n",
        "    tokens = nltk.word_tokenize(cleanline(sent).capitalize())\n",
        "    #lower-casing the tokens\n",
        "    tokens=[token.lower() for token in tokens]\n",
        "    #eliminating the stop words\n",
        "    nonstoptoken = [word for word in tokens if not word in stop_words] \n",
        "    tags = nltk.pos_tag(nonstoptoken)\n",
        "    #collecting the tokens with POS tag in categories nouns, adjectives and adverbs\n",
        "    tochange=[tag for tag in tags if tag[1][0] in ['J','N','R']]\n",
        "    nchange=[round(per*len(tochange)) for per in perfix] #number of required replacements due to the percentage given\n",
        "    #print(\"Len to change: {}\".format(len(tochange)))\n",
        "    #print(\"Nchange: {}\".format(nchange[0]))\n",
        "    linemodtotal=sent.lower()\n",
        "    indicator=sent[0].isupper()\n",
        "    changed=0\n",
        "    #adding the sentences into list if no change is supposed to be performed\n",
        "    if nchange[0]==0:\n",
        "        if (indicator):\n",
        "            toadd=linemodtotal.capitalize()\n",
        "        else:\n",
        "            toadd=linemodtotal\n",
        "        noisesent6.append(toadd)\n",
        "    index=-1\n",
        "    #the replacement order of the tokens could also be randomized by making the following line uncommented\n",
        "    #random.shuffle(tochange)\n",
        "    while(changed<nchange[-1]):\n",
        "        index+=1\n",
        "        if(index==len(tochange)):\n",
        "            if changed<nchange[0]:\n",
        "              if (indicator):\n",
        "                  toadd=linemodtotal.replace('#','').capitalize()\n",
        "              else:\n",
        "                  toadd=linemodtotal.replace('#','')\n",
        "              noisesent6.append(toadd)\n",
        "            break\n",
        "        word=tochange[index][0]\n",
        "        #print(\"Word to change: {}\".format(word))\n",
        "        #Obtaining the candidates for the replacement\n",
        "        [wn,check2]=synonyms(tochange[index][0],15)\n",
        "        #if the list returned is not empty\n",
        "        if (check2):\n",
        "            #candidate is randomly chosen\n",
        "            toreplace=wn[random.randint(0,len(wn)-1)]\n",
        "            #if the candidate chosen contains the character \"_\" it is replaced with a space character\n",
        "            if '_' in toreplace:\n",
        "                toreplace=toreplace.replace('_',' ')\n",
        "            #print(\"TOREPLACE: {}\".format(toreplace))\n",
        "            #word is replaced with the candidate accompanying \"#\" characters as boundaries\n",
        "            #this is done to prevent the replacement of the in-word and multiple occurences of the words in process \n",
        "            pattern='(?<![a-zA-Z#])'+word+'(?![a-zA-Z#])'\n",
        "            linemodtotal=re.sub(pattern,'#'+toreplace+'#',linemodtotal,1)\n",
        "            #print(linemodtotal)\n",
        "            changed+=1\n",
        "        else:\n",
        "            continue  \n",
        "        #f the desired number of words are replaced\n",
        "        if changed==nchange[0]:\n",
        "            #indicator serves as a sign of the capitalized sentence\n",
        "            #the \"#\" characters are removed at the end\n",
        "            if (indicator):\n",
        "                toadd=linemodtotal.replace('#','').capitalize()\n",
        "            else:\n",
        "                toadd=linemodtotal.replace('#','')\n",
        "            #the augmented sentences added to the list defined at the beginning\n",
        "            noisesent6.append(toadd)\n",
        "            break\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the path where the document containing the augmented samples will be saved to\n",
        "FINALPATH=\"/content/\"\n",
        "\n",
        "noiselist=[noisesent6]\n",
        "for element in noiselist:\n",
        "    folder=\"0.6\" #Could also bi iterated due to percentages. Here we use only one percentage value therefore it is strictly defined \n",
        "    final=[]\n",
        "    start=0\n",
        "    for i in range(0,len(nsentence)):\n",
        "        toaddm=''\n",
        "        n=nsentence[i]\n",
        "        #merging the sentences that are contained in the same maxim\n",
        "        for j in range(start,start+n):\n",
        "            toaddm=toaddm+' '+element[j]\n",
        "        toaddm=toaddm[1:]\n",
        "        final.append(toaddm)\n",
        "        start=start+n \n",
        "    #saving the augmented samples into a text document in the given final path\n",
        "    f=open(FINALPATH+'/maximsamples_wordnet'+folder[-1]+'0.txt', 'w',encoding=\"utf8\")\n",
        "    for line in final:\n",
        "        f.write(line+'\\n')\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "PhVWoN3BUcuP"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}