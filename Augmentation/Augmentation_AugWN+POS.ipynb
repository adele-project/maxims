{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Mounting the drive content that contains the necessary files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrsfcePutlgJ",
        "outputId": "8b85088e-e13a-4d6e-d79c-91f45ee9710a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#path of the file that contains the maxims to be augmented\n",
        "TRAINSETPATHX='/content/drive/MyDrive/Maximsamples.txt'\n",
        "\n",
        "f=open(TRAINSETPATHX,'r',encoding='utf8')\n",
        "maximbits=f.readlines()\n",
        "f.close()\n",
        "\n",
        "#removing the endline characters\n",
        "maximbits=[maxim[:-1] for maxim in maximbits]\n",
        "\n",
        "print(\"The number of maxims: {}\".format(len(maximbits)))"
      ],
      "metadata": {
        "id": "If2TlIDctnKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2acd8c84-dc23-4ad0-b302-9efc0cade5df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of maxims: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the necessary modules\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "import string\n",
        "from nltk.corpus import stopwords,wordnet\n",
        "import numpy as np\n",
        "import re\n",
        "import random"
      ],
      "metadata": {
        "id": "Be7kph18nmo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aa18000-7fdc-4253-c1f9-eb6b44bf6010"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#obtaining the list of stop words in english\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "EKlyVwG1kVgU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function that finds the similar words to the input using the WordNet and POS tags on 2 different levels\n",
        "def synonyms(wordgiven, ndesired):\n",
        "    #collecting the synsets from the WordNet interface\n",
        "    wn=wordnet.synsets(wordgiven)\n",
        "    words=[]\n",
        "    taglist=[]\n",
        "    #obtaning the POS tag of the word in process\n",
        "    wordtag=nltk.pos_tag([wordgiven])[0][1][0]\n",
        "    check=1\n",
        "    #if the result returned from the wordnet interface is not empty\n",
        "    if len(wn)!=0:\n",
        "        for word in wn:\n",
        "            #performing a POS compatibility check on the synset level that result in a prioritization tag\n",
        "            for entry in word.lemmas():\n",
        "                if wordtag=='N' and str(entry).split('.')[1]=='n':\n",
        "                    priortag=1\n",
        "                elif (wordtag in ['J']) and (str(entry).split('.')[1] in ['a','s']):\n",
        "                    priortag=1\n",
        "                elif (wordtag in ['R']) and (str(entry).split('.')[1] in ['r']):\n",
        "                    priortag=1\n",
        "                else:\n",
        "                    priortag=0\n",
        "                if (wordgiven.lower()!=entry.name().lower()) and (entry.name().lower() not in words):\n",
        "                    words.append(entry.name().lower()) #list that contains the results\n",
        "                    taglist.append(priortag) #list that contains the prioritization tags of the first check\n",
        "    #reordering of the results in the list based on the first prioritization tags obtained\n",
        "    #and saving the rioritization tags of the second check\n",
        "    wordsfinal=[None]*len(words) \n",
        "    if len(words)!=0:\n",
        "        indexstart=taglist.count(1) \n",
        "        count1=0 \n",
        "        indicate=[None]*len(words) #list that contains the prioritization tags of the second check\n",
        "        for i in range(0,len(words)):\n",
        "            if taglist[i]==1:\n",
        "                wordsfinal[count1]=words[i]\n",
        "                #determining the second indicators\n",
        "                if wordtag==nltk.pos_tag([words[i]])[0][1][0]:\n",
        "                    indicate[count1]=1\n",
        "                else:\n",
        "                    indicate[count1]=0\n",
        "                count1+=1\n",
        "            else:\n",
        "                #placing the unprioritized words at the lower indices\n",
        "                wordsfinal[indexstart]=words[i]\n",
        "                if wordtag==nltk.pos_tag([words[i]])[0][1][0]:\n",
        "                    indicate[indexstart]=1\n",
        "                else:\n",
        "                    indicate[indexstart]=0\n",
        "                indexstart+=1\n",
        "        #reordering of the results in the list based on the second prioritization tags obtained\n",
        "        wordsfinal2=[None]*len(wordsfinal)\n",
        "        indexstart1_2=indicate[0:taglist.count(1)].count(1) \n",
        "        count2=0 \n",
        "        #reordering of the already prioritized words (at the first check) based on the second prioritization tags obtained\n",
        "        for i in range(0,taglist.count(1)):\n",
        "            if indicate[i]==1:\n",
        "                wordsfinal2[count2]=wordsfinal[i]\n",
        "                count2+=1\n",
        "            else:\n",
        "                wordsfinal2[indexstart1_2]=wordsfinal[i]\n",
        "                indexstart1_2+=1\n",
        "        #reordering of the unprioritized words (at the first check) based on the second prioritization tags obtained\n",
        "        indexstart1_2=indicate[taglist.count(1):].count(1) \n",
        "        count2=taglist.count(1)\n",
        "        for i in range(taglist.count(1),len(wordsfinal)):\n",
        "            if indicate[i]==1:\n",
        "                wordsfinal2[count2]=wordsfinal[i]\n",
        "                count2+=1\n",
        "            else:\n",
        "                wordsfinal2[indexstart1_2]=wordsfinal[i]\n",
        "                indexstart1_2+=1  \n",
        "        check=1\n",
        "    else:\n",
        "        #returning an empty list if the number of results is equal to zero\n",
        "        wordsfinal2=wordsfinal\n",
        "        check=0\n",
        "    return wordsfinal2[0:ndesired],check   "
      ],
      "metadata": {
        "id": "M4rrudOJt-og"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function that cleans the lines from undesired characters as tags,punctuation signs etc.\n",
        "def cleanline(line):\n",
        "    tagpattern=r'(<)(.+?)(>)'\n",
        "    numberpattern=r'([0-9]+)'\n",
        "    cl=re.sub(tagpattern,'',line.lower()).replace('\\n','').replace('“','').replace('”','').replace('…','').replace('—',' ').replace('‘','').replace('’','').replace('\\\\' , ' ').replace('/',' ')\n",
        "    cl=re.sub(numberpattern,' ',cl)\n",
        "    for char in cl:\n",
        "            if (char in string.punctuation):\n",
        "                cl=cl.replace(char,' ')\n",
        "    return cl"
      ],
      "metadata": {
        "id": "UAYm0WnnlUiR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#breaking th maxim lines into sentences\n",
        "sentences=[]\n",
        "nsentence=[] #storing the number of sentences each maxim contains\n",
        "for line in maximbits:\n",
        "    count=0\n",
        "    #splitting sentences using \". \" characters\n",
        "    #listsent=line.replace('\\\\\\n','').split('. ')\n",
        "    listsent=line.split('. ')\n",
        "    for i in range(0,len(listsent)):\n",
        "        sentence=listsent[i]\n",
        "        #sentences that are too short are eliminated\n",
        "        if len(sentence)<5:\n",
        "            continue\n",
        "        elif (i==len(listsent)-1):\n",
        "            #the last one is added as it is since we used the character sequence \". \" for the seperation\n",
        "            #and the last sentence contains only \".\"\n",
        "            sentences.append(sentence)\n",
        "            count=count+1\n",
        "            continue\n",
        "        else:\n",
        "                        #the \".\" characters are restored\n",
        "            if(sentence[-1]!='.'):\n",
        "                sentence=sentence+'.'\n",
        "            sentences.append(sentence)\n",
        "            count=count+1\n",
        "    nsentence.append(count)"
      ],
      "metadata": {
        "id": "h3M7AhrwuTeY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sentences))\n",
        "print(len(nsentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USMcJXl7l8JA",
        "outputId": "fc70dba7-f568-4d8a-a165-d64213a05a4a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "m1swewYPq-kG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10358ffa-8e6a-4f11-dae7-0fc9337baea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n"
          ]
        }
      ],
      "source": [
        "perfix=[0.6] #percentage desired\n",
        "noisesent6=[]\n",
        "for sent in sentences:\n",
        "    print(sentences.index(sent))\n",
        "    #tokinizing and capitalizing the sentences\n",
        "    tokens = nltk.word_tokenize(cleanline(sent).capitalize())\n",
        "    #lower-casing the tokens\n",
        "    tokens=[token.lower() for token in tokens]\n",
        "    #eliminating the stop words\n",
        "    nonstoptoken = [word for word in tokens if not word in stop_words]\n",
        "    tags = nltk.pos_tag(nonstoptoken)\n",
        "    #collecting the tokens with POS tag in categories nouns, adjectives and adverbs\n",
        "    tochange=[tag for tag in tags if tag[1][0] in ['J','N','R']]\n",
        "    #print(tochange)\n",
        "    #print(\"Len to change: {}\".format(len(tochange)))\n",
        "    nchange=[round(per*len(tochange)) for per in perfix] #number of required replacements due to the percentage given\n",
        "    #print(\"Nchange: {}\".format(nchange[0]))\n",
        "    linemodtotal=sent.lower()\n",
        "    indicator=sent[0].isupper()\n",
        "    changed=0\n",
        "    index=-1\n",
        "    #the replacement order of the tokens could also be randomized by making the following line uncommented\n",
        "    #random.shuffle(tochange)\n",
        "    while(changed<nchange[-1]):\n",
        "        index+=1\n",
        "        if(index==len(tochange)):\n",
        "            if changed<nchange[0]:\n",
        "              if (indicator):\n",
        "                  toadd=linemodtotal.replace('#','').capitalize()\n",
        "              else:\n",
        "                  toadd=linemodtotal.replace('#','')\n",
        "              noisesent6.append(toadd)\n",
        "              print(toadd)\n",
        "            break\n",
        "        word=tochange[index][0]\n",
        "        #print(\"Word to change: {}\".format(word))\n",
        "        #Obtaining the candidates for the replacement\n",
        "        [wn,check2]=synonyms(tochange[index][0],5)\n",
        "        #if the list returned is not empty\n",
        "        if (check2):\n",
        "            #first candidate is chosen\n",
        "            toreplace=wn[0]\n",
        "            #if the candidate chosen contains the character \"_\" it is replaced with a space character\n",
        "            if '_' in toreplace:\n",
        "                toreplace=toreplace.replace('_',' ')\n",
        "            #print(\"TOREPLACE: {}\".format(toreplace))\n",
        "            #word is replaced with the candidate accompanying \"#\" characters as boundaries\n",
        "            #this is done to prevent the replacement of the in-word and multiple occurences of the words in process \n",
        "            pattern='(?<![a-zA-Z#])'+word+'(?![a-zA-Z#])'\n",
        "            linemodtotal=re.sub(pattern,'#'+toreplace+'#',linemodtotal,1)\n",
        "            #print(linemodtotal)\n",
        "            changed+=1\n",
        "        else:\n",
        "            continue\n",
        "        #if the desired number of words are replaced\n",
        "        if changed==nchange[0]:\n",
        "            #indicator serves as a sign of the capitalized sentence\n",
        "            #the \"#\" characters are removed at the end\n",
        "            if (indicator):\n",
        "                toadd=linemodtotal.replace('#','').capitalize()\n",
        "            else:\n",
        "                toadd=linemodtotal.replace('#','')\n",
        "            #the augmented sentences added to the list defined at the beginning\n",
        "            noisesent6.append(toadd)\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the path where the document containing the augmented samples will be saved to\n",
        "FINALPATH=\"/content/\"\n",
        "\n",
        "noiselist=[noisesent6]\n",
        "pername=[\"0.6\"]\n",
        "for index in range(0,len(noiselist)):\n",
        "    augmentedset=noiselist[index]\n",
        "    #stating the percentage value name that the document containing the augmented maxims will be saved\n",
        "    folder=pername[index]\n",
        "    #array that contains the augmented maxims\n",
        "    final=[]\n",
        "    start=0\n",
        "    #merging the sentences that belong to the same maxim again\n",
        "    for i in range(0,len(nsentence)):\n",
        "        toaddm=''\n",
        "        n=nsentence[i]\n",
        "        for j in range(start,start+n):\n",
        "            toaddm=toaddm+' '+augmentedset[j]\n",
        "        toaddm=toaddm[1:]\n",
        "        final.append(toaddm)\n",
        "        start=start+n \n",
        "    #saving the augmented samples into a text document in the given final path\n",
        "    f=open(FINALPATH+'/maximsamples_wordnet2pos'+folder[-1]+'0.txt', 'w',encoding=\"utf8\")\n",
        "    for line in final:\n",
        "        f.write(line+'\\n')\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "t9H5MFMluWu7"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}