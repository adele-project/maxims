{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#Mounting the drive content that contains the necessary files\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HGdbTLRC3hO1","outputId":"426e9188-fb67-411d-dfa7-6254e71983a6","executionInfo":{"status":"ok","timestamp":1669929288653,"user_tz":-180,"elapsed":74368,"user":{"displayName":"Sezen Perçin","userId":"03155805311209605338"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"8SnAeN5WYgv-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c70fa7f3-a98b-4a44-bcb5-a05e87c3e14f","executionInfo":{"status":"ok","timestamp":1669929475905,"user_tz":-180,"elapsed":187271,"user":{"displayName":"Sezen Perçin","userId":"03155805311209605338"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-12-01 21:14:48--  https://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2022-12-01 21:14:48--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 38s  \n","\n","2022-12-01 21:17:27 (5.19 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n","\n","Archive:  glove.6B.zip\n","  inflating: glove.6B.50d.txt        \n","  inflating: glove.6B.100d.txt       \n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n"]}],"source":["#Retrieving the pretrained Glove word embeddings which are pretrained on:\n","#Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, 100d)\n","#Source: https://nlp.stanford.edu/projects/glove/\n","\n","!wget https://nlp.stanford.edu/data/glove.6B.zip\n","!unzip glove.6B.zip"]},{"cell_type":"code","source":["#importing the necessary modules\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","from nltk.corpus import stopwords,wordnet\n","import string\n","import numpy as np\n","import re\n","import random\n","from sklearn.metrics.pairwise import cosine_similarity"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wEFFRmtSxwhr","outputId":"4f22783d-9a4b-4672-f0bf-8e2649a57e80","executionInfo":{"status":"ok","timestamp":1669929479252,"user_tz":-180,"elapsed":3417,"user":{"displayName":"Sezen Perçin","userId":"03155805311209605338"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]}]},{"cell_type":"code","source":["#obtaining the list of stop words in english\n","stop_words = set(stopwords.words('english'))\n","\n","#storing the word embeddings as a dictionary\n","embeddings={}\n","f_glove=open('/content/glove.6B.100d.txt', 'r',encoding=\"utf8\")\n","\n","for line in f_glove:\n","    entries=line.split()\n","    wordvector = np.asarray(entries[1:], \"float32\")\n","    embeddings[entries[0]]=wordvector\n","f_glove.close()"],"metadata":{"id":"-_wv3xFcyAVM","executionInfo":{"status":"ok","timestamp":1669929488009,"user_tz":-180,"elapsed":8765,"user":{"displayName":"Sezen Perçin","userId":"03155805311209605338"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#function that determines the most similar word among the ones collected from the WordNet interface via synonyms() function using the GloVe word embeddings\n","def mostsimilar(embeddings,word,candidates):\n","    scores=[]\n","    cand_score=[]\n","    check=0\n","    for element in candidates:\n","        cand=element.lower()\n","        #if the candidate contains a \"-\" or \"_\" character:\n","        if (len(cand.split('_'))>1) or (len(cand.split('-'))>1):\n","            count=0\n","            sum=0\n","            #it is replaced by a space character\n","            cand=cand.replace('_',' ').replace('-',' ')\n","            #For each part that the candidate is composed of:\n","            for part in cand.split(' '):\n","                #the word embedding is found if it exists\n","                try:\n","                    sum+=embeddings[part]\n","                    count+=1\n","                except:\n","                    continue\n","            #if no embedding was found none of the parts the procedure returns the first element on the candidate list given as input\n","            if (type(sum)==int):\n","                continue\n","            else:\n","                #if there is an existing embedding then the average of embedding that belongs to the part are found and serves as a score\n","                try:\n","                  embedding_cand=sum/count\n","                  cos=(np.dot(embedding_cand,embeddings[word])/np.linalg.norm(embedding_cand))/np.linalg.norm(embeddings[word])\n","                  scores.append(cos)\n","                  cand_score.append(element)\n","                  check+=1\n","                except:\n","                  continue\n","        else:\n","            #the same procedure\n","            try:\n","                embedding_cand=embeddings[cand]\n","                cos=(np.dot(embedding_cand,embeddings[word])/np.linalg.norm(embedding_cand))/np.linalg.norm(embeddings[word])\n","                scores.append(cos)\n","                cand_score.append(element)\n","                check+=1\n","            except:\n","                continue\n","    #if no embedding was found:\n","    if len(cand_score)==0:\n","        return wn[0]\n","    #scores and the corresponding candidates are merged together and resultant list is sorted\n","    s=np.array([np.array(cand_score),np.array(scores)],dtype=object)\n","    s=np.transpose(s)\n","    s=s[np.argsort(s[:,1],axis=0),:][::-1]   \n","    if '_' in s[0][0]:\n","        s[0][0]=s[0][0].replace('_',' ')\n","    #first element that has the highest score is returned                 \n","    return s[0][0]"],"metadata":{"id":"XpH8oLLqyMMp","executionInfo":{"status":"ok","timestamp":1669929488010,"user_tz":-180,"elapsed":53,"user":{"displayName":"Sezen Perçin","userId":"03155805311209605338"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#function that collects the candidates that are similar to the input word\n","def synonyms(wordgiven, ndesired):\n","    #collecting the synsets from the WordNet interface\n","    wn=wordnet.synsets(wordgiven)\n","    words=[]\n","    taglist=[]\n","    #obtaning the POS tag of the word in process\n","    wordtag=nltk.pos_tag([wordgiven])[0][1][0]\n","    check=1\n","    if len(wn)!=0:\n","        for word in wn:\n","            #performing a POS compatibility check on the synset level that result in a prioritization tag\n","            for entry in word.lemmas():\n","                if wordtag=='N' and str(entry).split('.')[1]=='n':\n","                    priortag=1\n","                elif (wordtag in ['J']) and (str(entry).split('.')[1] in ['a','s']):\n","                    priortag=1\n","                elif (wordtag in ['R']) and (str(entry).split('.')[1] in ['r']):\n","                    priortag=1\n","                else:\n","                    priortag=0\n","                if (wordgiven.lower()!=entry.name().lower()) and (entry.name().lower() not in words):\n","                    words.append(entry.name().lower()) #list that contains the results\n","                    taglist.append(priortag) #list that contains the prioritization tags\n","    #reordering of the results in the list based on their prioritization tag\n","    wordsfinal=[None]*len(words)\n","    if len(words)!=0:\n","        indexstart=taglist.count(1)\n","        count1=0\n","        for i in range(0,len(words)):\n","            if taglist[i]==1:\n","                wordsfinal[count1]=words[i]\n","                count1+=1\n","            else:\n","                wordsfinal[indexstart]=words[i]\n","                indexstart+=1\n","        check=1\n","    else:\n","        check=0\n","    return wordsfinal[0:ndesired],check  "],"metadata":{"id":"qOP9sOZOyPUc","executionInfo":{"status":"ok","timestamp":1669929488011,"user_tz":-180,"elapsed":51,"user":{"displayName":"Sezen Perçin","userId":"03155805311209605338"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["#function that cleans the lines from undesired characters as tags,punctuation signs etc.\n","def cleanline(line):\n","    tagpattern=r'(<)(.+?)(>)'\n","    numberpattern=r'([0-9]+)'\n","    cl=re.sub(tagpattern,'',line.lower()).replace('\\n','').replace('“','').replace('”','').replace('…','').replace('—',' ').replace('‘','').replace('’','').replace('\\\\' , ' ').replace('/',' ')\n","    cl=re.sub(numberpattern,' ',cl)\n","    for char in cl:\n","            if (char in string.punctuation):\n","                cl=cl.replace(char,' ')\n","    return cl"],"metadata":{"id":"7kevD-12yn76","executionInfo":{"status":"ok","timestamp":1669929488012,"user_tz":-180,"elapsed":50,"user":{"displayName":"Sezen Perçin","userId":"03155805311209605338"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["#path of the file that contains the maxims to be augmented\n","TRAINSETPATHX='/content/drive/MyDrive/Maximsamples.txt'\n","\n","f=open(TRAINSETPATHX,'r',encoding='utf8')\n","maximbits=f.readlines()\n","f.close()\n","#removing the endline characters\n","maximbits=[maxim[:-1] for maxim in maximbits]\n","\n","print(\"The number of maxims: {}\".format(len(maximbits)))"],"metadata":{"id":"rR__QjpDymN5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669929488013,"user_tz":-180,"elapsed":48,"user":{"displayName":"Sezen Perçin","userId":"03155805311209605338"}},"outputId":"b41e8d89-7bf0-427b-c4be-257f612ad606"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of maxims: 10\n"]}]},{"cell_type":"code","source":["maximbits"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fVUuH4laYG9V","outputId":"cc431ec5-3800-40c1-ff3f-1f2bda971625","executionInfo":{"status":"ok","timestamp":1669929488364,"user_tz":-180,"elapsed":389,"user":{"displayName":"Sezen Perçin","userId":"03155805311209605338"}}},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Lastly the rules which must make up the reference system should be identified according to objective criteria in particular to enable judicial review of the assessments on which that identification is based. It is for the Commission to take into account any factors put forward by the Member State concerned and more generally to carry out its examination in a rigorous and sufficiently reasoned manner in order to enable full judicial review.',\n"," 'Second it is for the Commission when it approves a general aid scheme to take the necessary measures to ensure that the sectoral rules other than the general competition rules will be complied with by the Member State concerned.',\n"," 'Having regard to the foregoing considerations the answer to be given to the first question must be that national measures which provide for a rebate of energy taxes on natural gas and electricity only in the case of undertakings whose activity is shown to consist primarily in the manufacture of goods must be regarded as State aid within the meaning of Article of the Treaty.',\n"," 'Contrary to what was held by the General Court in the judgments under appeal neither can it be required of the Commission in order to establish the selectivity of such a measure that it should identify certain specific features that are characteristic of and common to the undertakings that are the recipients of the tax advantage by which they can be distinguished from those undertakings that are excluded from the advantage.',\n"," 'The concept of aid is more general than that of a subsidy. It embraces not only positive benefits but also measures which in various forms mitigate the charges which are normally included in the budget of an undertaking and which without therefore being subsidies in the strict meaning of the word are similar in character and have the same effect (Case / De Gezamenlijke Steenkolenmijnen in Limburg v High Authority [] ECR ; Case C-/ Banco Exterior de Espana [] ECR I- paragraph ; and Case C-/ Ecotrade [] ECR I- paragraph ).',\n"," 'As regards the first plea of inadmissibility raised by the Commission it must be rejected on the same grounds as those set out in paragraphs to above. A party is entitled to put forward pleas and arguments arising from the judgment under appeal itself and which seek to criticise in law its correctness. The appellant is therefore entitled to call into question the findings made by the General Court irrespective of the fact that it did not put forward at first instance arguments intended specifically to challenge the decision at issue on that point.',\n"," 'A State measure which benefits all undertakings in national territory without distinction cannot therefore constitute State aid.',\n"," 'In that regard the fact that an economic sector has been the subject of liberalisation at EU level may serve to determine that the aid in question has a real or potential effect on competition and affects trade between Member States (judgment of April Commission v Italy and Wam C‑/ P EU:C:: paragraph and the case-law cited).',\n"," 'Moreover for the purposes of applying the private creditor test the only relevant evidence is the information which was available and the developments which were foreseeable at the time when the decision was taken (see to that effect judgment of June Commission v EDF C‑/ P EU:C:: paragraph ).',\n"," 'It follows that the interveners were not entitled to raise the objection of inadmissibility and that the Court is therefore not bound to consider the pleas on which they rely.']"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["#breaking th maxim lines into sentences\n","sentences=[]\n","nsentence=[] #storing the number of sentences each maxim contains\n","for line in maximbits:\n","    count=0\n","    #splitting sentences using \". \" characters\n","    #listsent=line.replace('\\\\\\n','').split('. ')\n","    listsent=line.split('. ')\n","    for i in range(0,len(listsent)):\n","        sentence=listsent[i]\n","        #sentences that are too short are eliminated\n","        if len(sentence)<5:\n","            continue\n","        elif (i==len(listsent)-1):\n","            #the last one is added as it is since we used the character sequence \". \" for the seperation\n","            #and the last sentence contains only \".\"\n","            sentences.append(sentence)\n","            count=count+1\n","            continue\n","        else:\n","            #the \".\" characters are restored\n","            if(sentence[-1]!='.'):\n","                sentence=sentence+'.'\n","            sentences.append(sentence)\n","            count=count+1\n","    nsentence.append(count)"],"metadata":{"id":"28QVG-XMzwOX","executionInfo":{"status":"ok","timestamp":1669929488365,"user_tz":-180,"elapsed":18,"user":{"displayName":"Sezen Perçin","userId":"03155805311209605338"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["print(len(sentences))\n","print(len(nsentence))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D3fliAZ4YMpq","outputId":"956ac5a1-7e11-48a8-a665-59e249993a35","executionInfo":{"status":"ok","timestamp":1669929488365,"user_tz":-180,"elapsed":17,"user":{"displayName":"Sezen Perçin","userId":"03155805311209605338"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["14\n","10\n"]}]},{"cell_type":"code","source":["perfix=[0.6] #percentage desired\n","noisesent6=[]\n","for sent in sentences:\n","    print(sentences.index(sent))\n","    #tokinizing and capitalizing the sentences\n","    tokens = nltk.word_tokenize(cleanline(sent).capitalize())\n","    #lower-casing the tokens\n","    tokens=[token.lower() for token in tokens]\n","    #eliminating the stop words\n","    nonstoptoken = [word for word in tokens if not word in stop_words] \n","    tags = nltk.pos_tag(nonstoptoken)\n","    #collecting the tokens with POS tag in categories nouns, adjectives and adverbs\n","    tochange=[tag for tag in tags if tag[1][0] in ['J','N','R']]\n","    nchange=[round(per*len(tochange)) for per in perfix] #number of required replacements due to the percentage given\n","    #print(\"Len to change: {}\".format(len(tochange)))\n","    #print(\"Nchange: {}\".format(nchange[0]))\n","    linemodtotal=sent.lower()\n","    indicator=sent[0].isupper()\n","    changed=0\n","    #adding the sentences into list if no change is supposed to be performed\n","    if nchange[0]==0:\n","        if (indicator):\n","            toadd=linemodtotal.capitalize()\n","        else:\n","            toadd=linemodtotal\n","        noisesent6.append(toadd)\n","    index=-1\n","    #the replacement order of the tokens could also be randomized by making the following line uncommented\n","    #random.shuffle(tochange)\n","    while(changed<nchange[-1]):\n","        index+=1\n","        if(index==len(tochange)):\n","            if changed<nchange[0]:\n","              if (indicator):\n","                  toadd=linemodtotal.replace('#','').capitalize()\n","              else:\n","                  toadd=linemodtotal.replace('#','')\n","              noisesent6.append(toadd)\n","              #print(toadd)\n","            break\n","        word=tochange[index][0]\n","        #print(\"Word to change: {}\".format(word))\n","        #Obtaining the candidates for the replacement\n","        [wn,check2]=synonyms(tochange[index][0],10)\n","        #if the list is not empty:\n","        if (check2):\n","            toreplace=mostsimilar(embeddings,word,wn)\n","            #print(\"TOREPLACE: {}\".format(toreplace))\n","            #word is replaced with the candidate accompanying \"#\" characters as boundaries\n","            #this is done to prevent the replacement of the in-word and multiple occurences of the words in process \n","            pattern='(?<![a-zA-Z#])'+word+'(?![a-zA-Z#])'\n","            linemodtotal=re.sub(pattern,'#'+toreplace+'#',linemodtotal,1)\n","            #print(linemodtotal)\n","            changed+=1\n","        else:\n","            continue\n","        #if the desired number of words are replaced\n","        if changed==nchange[0]:\n","            #indicator serves as a sign of the capitalized sentence\n","            #the \"#\" characters are removed at the end\n","            if (indicator):\n","                toadd=linemodtotal.replace('#','').capitalize()\n","            else:\n","                toadd=linemodtotal.replace('#','')\n","            #the augmented sentences added to the list defined at the beginning\n","            noisesent6.append(toadd)\n","            break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1HSWmJ4qa0Vj","outputId":"a17a5ac2-4ce7-4bb1-9a24-7b10663a17ae","executionInfo":{"status":"ok","timestamp":1669929490517,"user_tz":-180,"elapsed":2159,"user":{"displayName":"Sezen Perçin","userId":"03155805311209605338"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n"]}]},{"cell_type":"code","source":["#the path where the document containing the augmented samples will be saved to\n","FINALPATH=\"/content/\"\n","\n","noiselist=[noisesent6]\n","for element in noiselist:\n","    folder=\"0.6\" #Could also bi iterated due to percentages. Here we use only one percentage value therefore it is strictly defined \n","    final=[]\n","    start=0\n","    for i in range(0,len(nsentence)):\n","        toaddm=''\n","        n=nsentence[i]\n","        #merging the sentences that are contained in the same maxim\n","        for j in range(start,start+n):\n","            toaddm=toaddm+' '+element[j]\n","        toaddm=toaddm[1:]\n","        final.append(toaddm)\n","        start=start+n\n","    #saving the augmented samples into a text document in the given final path\n","    f=open(FINALPATH+'/maximsamples_wordnetglove'+folder[-1]+'0.txt', 'w',encoding=\"utf8\")\n","    for line in final:\n","        f.write(line+'\\n')\n","    f.close()"],"metadata":{"id":"27YINnAI7mvs","executionInfo":{"status":"ok","timestamp":1669929490518,"user_tz":-180,"elapsed":18,"user":{"displayName":"Sezen Perçin","userId":"03155805311209605338"}}},"execution_count":13,"outputs":[]}]}